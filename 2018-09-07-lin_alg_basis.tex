\DiaryEntry{Why $\{1, x, x^2\}$ is a bad Basis}{2018-09-07}{Maths}


Based on the \href{https://www.youtube.com/watch?v=pYoGYQOXqTk&index=14&list=PL.}{YT Lecture}.

\subsection{Introduction}

First question is how to express a vector $\xbf=(x_1,x_2)^T$ in a non-orthogonal basis with vectors $\ubf_1, \ubf_2$ and coefficients $a_1, a_2$

\bee
\xbf = a_1 \ubf_1 + a_2 \ubf_2
\eee

We calculate the inner product with $\ubf_1$ and $\ubf_2$, repsectively and assume the basis vectors have length $1$ (i.e. $\ubf_i \cdot \ubf_i = 1$),

\begin{align*}
  \xbf \cdot \ubf_1 &= a_1 + a_2 \ubf_2 \cdot \ubf_2 \\
  \xbf \cdot \ubf_2 &= a_1 \ubf_1 \cdot \ubf_2 + a_2
\end{align*}

These are two equations with 2 unknowns and can be solved.

In the special case of an orthonormal basis, $\ubf_i \ubf_j = \delta_{i,j}$, we obtain

\begin{align*}
  \xbf \cdot \ubf_1 &= a_1 \\
  \xbf \cdot \ubf_2 &= a_2
\end{align*}

In words, the two basis coefficients are given by the inner product of the vector with the basis vectors.

As a concrete example for the general 2-dimensional case, we consider

\bee
\ubf_1 = (1,0)^T, \quad \ubf_2=(\cos \phi, \sin \phi)^T
\eee

and obtain the following two equations

\begin{align}\label{2018-09-07:eq1}
  x_1 &= a_1 + a_2 \cos \phi \\
  x_1 \cos \phi + x_2 \sin \phi &= a_1 \cos \phi + a_2 \nonumber
\end{align}

From the first equation we obtain $a_1 = x_1 - a_2 \cos \phi$ which we insert into the second equation,

\bee
x_1 \cos \phi + x_2 \sin \phi = (x_1 - a_2 \cos \phi) \cos \phi + a_2 \rightarrow \cdots \rightarrow a_2 = \frac{x_2}{\sin \phi}
\eee

Inserting back, we obtain

\bee
a_1 = x_1 - \frac{x_2}{\tan \phi}
\eee

For $\phi = \pi/2$, this becomes $a_1 = x_1$ and $a_2 = x_2$. In case of small angles $\phi$, both $\sin\phi$ and $\tan \phi$ become very small. Since both are in the denomiator, a small change in $x_2$ causes large changes if $a_1, a_2$; i.e. the basis expansion becomes very sensitive to movements of $\xbf$, which is not desirable (imagine that $\xbf$ is a noisy measurement). This can also be seen when we express \eqref{2018-09-07:eq1} in matrix form,

\bee
\Mbf = \begin{pmatrix} 1 & \cos \phi \\
  \cos \phi & 1
\end{pmatrix}
\eee

and calculate the matrix condition number. E.g., for $\phi=0.1$, we obtain a condition number of $\approx 400$ which is a rather high value and shows the high sensitivity.

\paragraph{Simulation Resuts\todo{simulate random pertubations around fixed x1, x2 and show variance of a1, a2 for different phi}}

\subsection{Polynomials (Legendre Polynomials)}

From the above discussion we see that an orthonormal basis (i) allows easy basis expansion and (ii) is not sensitive for errors. If we think of expressing a function by means of polynomials, we have lots of options for the ``basis functions''. One obvious choice would be $\Bc = \{1, x, x^2, x^3, \ldots\}$; however these functions are not orthogonal (on the interval $[-1;1]$

\bee
<x^m, x^n> = \int_{-1}^1 x^{m+n} dx = \left. \frac{x^{m+n+1}}{m+n+1}\right|_{-1}^1 = \begin{cases}
  \frac{2}{m+n+1}, & \quad m+n \, \text{even,} \\
  0 & \quad m+n \text{odd}
\end{cases}
\eee

The Legendre polynomials $P_n(x)$ are orthogonal on $[-1,1]$. There are several definitions for them, one is as solution for the Legendre differential equation,

\bee
\frac{d}{dx} \left[ (1-x^2) \frac{dP_n(x)}{dx}  \right] + n(n+1)P_n(x) = 0
\eee

with the ``initial'' condition $P_n(1) = 1$. A direct formula for the polynomials is

\bee
P_n(x) = \frac{1}{2^n n!}\frac{d^n}{dx^n}(x^2 - 1)^n
\eee

and a recursive definition is

\bee
(n+1)P_{n+1}(x) = (2n+1)xP_n (x) - nP_{n-1}(x)
\eee

todo\todo{Proofs for the above relations}

We also obtain the Legendre polynomials (up to a factor) when we perform a Gram-Schmidt orthogonalization on the set $1,x,x^2, \ldots$. The first two elements are already orthogonal,

\bee
<1,x> = \int_{-1}^1 x dx = 0
\eee

the third basis vector $\ubf_3$ becomes

\bee
\ubf_3 = \vbf_3 - \frac{\ubf_1, \vbf_3}{\ubf_1, \ubf_1} \ubf_1 - \frac{\ubf_2, \vbf_3}{\ubf_2, \ubf_2} \ubf_2
\eee

With $\ubf_1 = 1, \ubf_2 = x$, and $\vbf_3 = x^2$ we obtain

\bee
\ubf_3 = x^2 - \frac{<1,x^2>}{1,1}1 - \frac{<x,x^2>}{<x,x>}x = \cdots = \frac{1}{3}(3x^2-1)
\eee

Anyway, the first three Legendre polynomials are

\bee
P_0(x) = 1, P_1(x) = x, P_2(x) = \frac{1}{2}(3x^2-1), P_3(x) = \frac{1}{2}(5x^3 - 3x), \ldots
\eee

\paragraph{Using Maxima.} The Lengendre polynomials are available from Maxima; as an example, see the following session

\begin{verbatim}
load(orthopoly);
rat(legendre_p(3,x));
(5*x^3-3*x)/2
integrate(legendre_p(1,x)*legendre_p(2,x),x,-1,1);
0
\end{verbatim}

\subsection{Expressing a Function in terms of Legendre Polynoms}

We make the following Ansatz

\bee
f(x) = \sum_n a_n P_n(x)
\eee

Multiplying both sides with $P_m(x)$ and integrating yields

\bee
\int_{-1}^1 f(x) P_m(x) dx = \sum_n a_n \int_{-1}^1 P_m(x) P_n dx = \frac{2}{2m+1} a_m
\eee

and from this expression we can obtain an equation for the coefficients $a_m$,

\bee
a_m = \frac{2m+1}{2} \int_{-1}^1 f(x) P_m(x) dx
\eee

For the fun of it, let us express $f(x)=x^2$ in terms of Legendre polynomials. We have

\begin{align*}
  a_0 &= \frac{1}{2} \int_{-1}^1 x^2 \cdot 1 dx = \cdots = \frac{1}{3}\\
  a_1 &= \frac{3}{2} \int_{-1}^1 x^2 \cdot x dx = \cdots = 0\\
  a_2 &= \int_{-1}^1 x^2 \cdot \frac{1}{2}(3x^2-1) dx = \cdots = \frac{2}{3}
\end{align*}

We therefore obtain

\bee
f(x) = \frac{1}{3} \cdot 1 + 0 \cdot x + \frac{2}{3} \frac{1}{2}(3x^2-1) = \cdots = x^2 \qed
\eee

Here we have used the ``non-normalized'' Lengendre polynomials. We can define ``normed'' polynomials $\tilde{P}_m(x) = \sqrt{\frac{2m+1}{2}} P_m(x)$ an repeate the exercise. However, the results are not so nice (lots of square roots - both in the polynomials and the coefficients), while the result is the same.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "journal"
%%% End:
