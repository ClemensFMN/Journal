\DiaryEntry{Interpolation}{2025-06-12}{Maths}

Based on \cite{suli2003numericalanalysis}.

\subsection{Lagrange Interpolation}

We start with a definition: Given that $n$ is a non-negative integer, let $\Pc_n$ denote the set of all (real-valued) polynomials of degree $\leq n$ defined over the set of $\mR$, the real numbers.

We consider the following problem.

\begin{definition}
Let $n \geq 1$, and suppose that $x_i, i = 0, 1,\ldots,n$, are distinct real numbers (i.e., $x_i \neq x_j$ for $i \neq j$) and $y_i, i = 0, 1, \ldots , n$, are real numbers; we wish to find $p_n \in \Pc_n$ such that $p_n(x_i) = y_i, i = 0, 1,\ldots,n$.
\end{definition}


Define the function

\bee
L_k(x) = \prod_{i=0, i \neq k}^n \frac{x - x_i}{x_k - x_i}
\eee

From this definition follows immediately

\bee
L_k(x_i) = \begin{cases} 1, \quad i=k \\ 0, \quad i \neq k \end{cases}
\eee

We can then define the Lagrange interpolation polynomial of degree $n$ for the set of points $\{(x_i, y_i): i = 0,\ldots,n\}$ as

\be\label{2025-06-12:eq1}
p_n(x) = \sum_{k=0}^n L_k(x) y_k
\ee

One can show that this polynomial is unique if the interpolation conditions $p_n(x_i) = y_i, i = 0, 1,\ldots,n$ shall hold (intuitively it is clear as a polynomial of degree $n$ can obtain defined values at $n$ positions).

\paragraph{Example.} Let's consider $f(x) = e^x$ with interpolation points $x_0 = -1, x_1 = 0, x_2 = 1$.

As $n = 2$, our three $L$ functions become

\bee
L_0 = \frac{(x - x_1)(x - x_2)}{(x_0 - x_1)(x_0 - x_2)} = \frac{1}{2}x (x-1)
\eee

\bee
L_1 = \frac{(x - x_0)(x - x_2)}{(x_1 - x_0)(x_1 - x_2)} = 1 - x^2
\eee

and

\bee
L_2 = \frac{(x - x_0)(x - x_1)}{(x_1 - x_0)(x_1 - x_1)} = \frac{1}{2}x (x+1)
\eee

We can do this in Maxima

\begin{verbatim}
    L0:(x-x1)*(x-x2)/(x0-x1)/(x0-x2);
    subst(1,x2,subst(0,x1,subst(-1,x0,L0)));
    ((x-1)*x)/2
    L1:(x-x0)*(x-x2)/(x1-x0)/(x1-x2);
    subst(1,x2,subst(0,x1,subst(-1,x0,L1)));
    -((x-1)*(x+1))
    L2:(x-x0)*(x-x1)/(x2-x0)/(x2-x1);
    subst(1,x2,subst(0,x1,subst(-1,x0,L2)));
    (x*(x+1))/2
\end{verbatim}

Having obtained the $L$ functions, we can write the Lagrange polynomial \eqref{2025-06-12:eq1} as

\bee
p_2(x) = \frac{1}{2}x (x-1) e^{-1} + (1-x^2)e^0 + \frac{1}{2}x (x+1) e^1
\eee

The following Figure shows the plot of the function $e^x$ and the interpolation; note that both intersect at the interpolation points, but the interpolated function differs elsewhere. This shold not be surprising; we have added no further constraints (such as a minimization of the mean squared interpolation error) as we will do below.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{images/2025-06-12-exp.png}
\end{figure}

\paragraph{Another Example (Runge Oscillations).} Now we approximate the function

\bee
f(x) = \frac{1}{1+x^2}
\eee

in the interval $[-5,5]$ with $n$ points. Actually, there is a Maxima package which provides these polynomial interpolation functions and we will use this (so that we save ourselves the effort for constructing the $L-i$ functions).

We start with an interpolation polynomial at $x_0 = -5, x_1 = 0, x_2 = 5$.

\begin{verbatim}
    load(interpol);
    func:1/(1+x^2);
    f:makelist([x, 1/(1+x^2)], x, -5, 5, 5);
    lf: lagrange(f);
    plot2d([func, lf], [x,-5,5], [y,-1,1],
 [plot_format, gnuplot])$
\end{verbatim}

A plot of the function and its interpolation can be seen in the following Figure. We observe the same pattern as before: At the interpolation points, function and interpolation match; elsewhere there is a rather large interpolation error.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{images/2025-06-12-runge_1.png}
\end{figure}

We increase the number of interpolation points in the hope of reducing the interpolation error.

\begin{verbatim}
    \begin{verbatim}
    load(interpol);
    func:1/(1+x^2);
    f:makelist([x, 1/(1+x^2)], x, -5, 5, .5);
    lf: lagrange(f);
    plot2d([func, lf], [x,-5,5], [y,-1,1],
 [plot_format, gnuplot])$
\end{verbatim}

The following Figure shows again a comparison between the function and its interpolation. In some interval $[-2,2]$ the interpolation got much better; however outside the interval and towards the outer edges, the interpolation got much worse as the maximum error increased. This phenomenon is called \emph{Runge phenomenon}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{images/2025-06-12-runge_2.png}
\end{figure}


\subsection{Integration}

The Newton-Cotes formula shows how an integral can be approximated by a discrete sum,

\bee
\int_a^b f(x) dx \approx \int_a^b p_n(x) dx
\eee

If we insert the interpolation formulas from above and rename terms, we obtain the following approximation

\bee
\int_a^b f(x) dx \approx \sum_{k=0}^n w_k f(x_k)
\eee

with the weights $w_k$ given by

\bee
w_k = \int_a^b L_k(x) dx, \quad k=0,\ldots,n
\eee

The values $w_k$ are referred to as the quadrature weights, while the interpolation points $x_k$ are called the quadrature points. The numerical quadrature  rule quadrature weights and equally spaced quadrature points, is called the Newton-Cotes formula of order $n$. It is interesting to note that the quadratur weights $w_k$ do \emph{not} depend on the function to approximate; so the weights can be calculated and stored beforehand.

In order to illustrate the general idea, we consider two simple examples.

\paragraph{Example 1.} Consider the cae $n=1$. We have two interpolation points, $x_0 = a, x_1 = b$. The functions $L_0(x)$ and $L_1(x)$ take on the following simple forms,

\bee
L_0(x) = \frac{x-b}{a-b}, \quad L_1(x) = \frac{x-a}{b-a}
\eee

Calculating the weights, we obtain

\bee
w_0 = \int_a^b L_0(x) dx = \int_a^b \frac{x-b}{a-b} dx = \cdots = \frac{b-a}{2}
\eee

and

\bee
w_1 = \int_a^b L_1(x) dx = \int_a^b \frac{x-b}{b-a} dx = \cdots = \frac{b-a}{2}
\eee

and therefore the integral approximation becomes

\bee
\int_a^b f(x) dx \approx \sum_{k=0}^n w_k f(x_k) = \frac{b-a}{2} \left[ f(a) + f(b) \right]
\eee

This is the well-known trapezium rule.

\paragraph{Example 2.} Now let's move up one step and consider $n=2$. We have three interpolation points, $x_0 = a, x_1 = \frac{a+b}{2}, x_2 = b$. The function $L_0(x)$ becomes

\bee
L_0(x) = \frac{(x-x_1)(x-x_2)}{(x_0 - x_1)(x_0 - x_2)}
\eee

and we can integrate this to obtain

\bee
w_0 = \int_a^b L_0(x) dx = \cdots = \frac{b-a}{6}
\eee

In a similar spirit, we obtain $w_1 = \frac{4}{6}(b-a)$ and - by symmetry - $w_2 = w_0$. Therefore, the integral approximation becomes

\bee
\int_a^b f(x) dx \approx \sum_{k=0}^n w_k f(x_k) = \frac{b-a}{6} \left[ f(a) + 4 f\left( \frac{a+b}{2} \right) + f(b) \right]
\eee

which is Simpson's rule.

We can do this rather easily in Maxima; to simplify terms, we have to use \verb+ factor+ as follows.

\begin{verbatim}
L0:(x-(a+b)/2)*(x-b)/(a-(a+b)/2)/(a-b);
w0:integrate(L0,x,a,b);
factor(w0);
(b-a)/6    
\end{verbatim}

In a similar spirit we can also calculate / simplify $w_1$ and $w_2$.


\subsection{Hermite Interpolation}

We can extend above interpolation idea and extend it to find a polynomial $p_{2n+1}(x)$ which matches the function at the interpolation points and its derivative,

\bee
p_{2n+1}(x_i) = y_i, \quad p'_{2n+1}(x) = z_i, \quad i = 0, \ldots, n
\eee

It can be shown (Hermite interpolation theorem), that such a polynomial exists, it is unique, and has the following form

\begin{equation*}
    p_{2n+1}(x) = \sum_{k=0}^n \left[ H_k(x) y_k + K_k(x) z_k \right]
\end{equation*}

The auxiliary polynomials are defined as

\begin{equation*}
    H_k(x) = L_k^2(x) \left( 1 - 2 L'_k(x_k)(x-x_k) \right)
\end{equation*}

and

\begin{equation*}
    K_k(x) = L_k^2(x) (x-x_k)
\end{equation*}

with the polynomial $L_k(x)$ defined as before

\bee
L_k(x) = \prod_{i=0, i \neq k}^n \frac{x - x_i}{x_k - x_i}
\eee

From these definitions follows

 \begin{equation*}
    H_k(x_i) = \begin{cases} 1, \quad i=k \\ 0, \quad i \neq k \end{cases}, \quad H'_k(x_i) = 0, \quad i,k=0, 1, \ldots n
 \end{equation*}

and

\begin{equation*}
    K_k(x_i) = 0, \quad K'_k(x_i) = \begin{cases} 1, \quad i=k \\ 0, \quad i \neq k \end{cases}, \quad i,k=0, 1, \ldots n
\end{equation*}


\subsection{Approximation in the 2-norm}

The inner product between two functions is defined as

\bee
\langle f, g \rangle = \left( \int_a^b w(x) f(x) g(x) \right)^{1/2}
\eee

This induces a 2-norm

\bee
\| f  \|_2 = \left( \int_a^b w(x) f^2(x) \right)^{1/2}
\eee


\subsubsection{Orthogonal Polynomials}

Given a weight function $w$, defined, positive, continuous and integrable on the interval $(a, b)$, we say that the sequence of polynomials $\phi_j , j = 0, 1, \ldots$ is a system of orthogonal polynomials on the interval $(a, b)$ with respect to $w$, if each $\phi_j$ is of exact degree $j$, and
if

\bee
\int_a^b w(x) \phi_j(x) \phi_k(x) dx \begin{cases} = 0 \quad \forall j \neq k \\ \neq 0 \quad \forall j = k \end{cases}
\eee

We can use the Gram-Schmidt orthogonalization procedure to obtain a sequence of orthogonal polynomials. If we chose $w(x) = 1$ and $\phi_0(x) = 1$, then the procedure yields the Legendre polynomials.

The first four are:

\begin{align*}
    \phi_0(x) &= 0 \\
    \phi_1(x) &= x \\
    \phi_2(x) &= \frac{3}{2}x^2 - \frac{1}{2} \\
    \phi_3(x) &= \frac{5}{2}x^3 - \frac{3}{2}x \\
\end{align*}

We can obtain these in Maxima using \verb+ expand(legendre_p(p,x)); + .

\subsubsection{Approximation}

The underlying idea for approximation is the projection theorem: We project a function $f(x)$ onto a subspace $\Sc$ and let $f_\Sc(x)$ denote the projection. Then the best approximation (in the $2$-norm sense) is achieved if the appromation error $f - f_\Sc$ is orthogonal to the subspace $\Sc$.

Interestingly, when we use orthogonal polynomials, the best approximation is unique.

\begin{definition}
There exists a unique polynomial $p_n \in \Pc_n$ such that $\| f - p_n \|_2 = \min_{q \in \Pc_n} \| f-q \|_2$.    
\end{definition}

To actually get the approximation, we first normalize the polynomials to obtain 

\bee
\psi_k(x) = \frac{\phi_k(x)}{\| phi_k \|_2}
\eee

Then we calculate the coefficients

\bee
\beta_k = \langle f, \psi_k \rangle
\eee

and obtain the approximation according to

\bee
p_n(x) = \beta_0 \psi_0(x) + \beta_1 \psi_1(x) + \cdots
\eee

\paragraph{Example.} As a simple example, we approximate $e^x$ in the interval $[-1,1]$ for $n = 3$ in Maxima. The following shows a (shortened) Maxima session obtaining the approximation.

\begin{verbatim}
L0:legendre_p(0,x) / sqrt(integrate(legendre_p(0,x)^2, x,-1,1));
L1:legendre_p(1,x) / sqrt(integrate(legendre_p(1,x)^2, x,-1,1));
L2:legendre_p(2,x) / sqrt(integrate(legendre_p(2,x)^2, x,-1,1));
L3:legendre_p(2,x) / sqrt(integrate(legendre_p(3,x)^2, x,-1,1));
f:exp(x);
c0:integrate(f*L0, x, -1, 1);
c1:integrate(f*L1, x, -1, 1);
c2:integrate(f*L2, x, -1, 1);
c3:integrate(f*L3, x, -1, 1);
p:c0*L0 + c1*L1 + c2*L2 + c3*L3;
\end{verbatim}

The Figure shows the approximation together with $e^x$; different to before, there are now no interpolation points where the function and its approximation match.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{images/2025-06-12-exp_legendre.png}
\end{figure}


\subsection{Gauss Quadrature Rules}




%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=0.75]{images/2025-06-12-exp.png}
%\end{figure}


