\DiaryEntry{Linear Algebra - QR Decomposition}{2019-11-22}{Linear Algebra}

The underlying idea of the QR decomposition is to factor a matrix $\Abf$ into $\Abf = \Qbf \Rbf$, where $\Qbf$ is an othogonal matrix and $\Rbf$ is an upper triangular matrix.

Writing $\Abf$ as a product $\Qbf \Rbf$ allows solving a linear equation system in a numercially stabel way: The equation $\Abf \xbf = \abf$ becomes $\Qbf \Rbf \xbf = \abf$ and from this we obtain $\Rbf \xbf = \Qbf^T \abf$ and this equation can be solved easily via back-substitution.


\paragraph{Square Matrix.} We have $\Abf = \Qbf \Rbf$ with matrix $\Abf$ being an $n \times n$ matrix. If $\Abf$ is invertible (and therefore full-rank), then the factorization is unique if we require the diagonal elements of $\Rbf$ to be positive.

If $\Abf$ has $n$ linearly independent columns, then the first $n$ columns of $\Qbf$ form an orthonormal basis for the column space of $\Abf$. More generally, the first $k$ columns of $\Qbf$ form an orthonormal basis for the span of the first $k$ columns of $\Abf$ for any $1 \leq k \leq n$. The fact that any column $k$ of $\Abf$ only depends on the first $k$ columns of $\Qbf$ is responsible for the triangular form of $\Rbf$.

\paragraph{Rectangular Matrix.} In this case we can factor an $m \times n$ matrix $\Abf$, with $m \geq n$, as the product of an $m \times m$ unitary matrix $\Qbf$ and an $m \times n$ upper triangular matrix $\Rbf$. The bottom $m-n$ rows of $\Rbf$ consist of zeros; we therefore can write the QR decomposition in a partitioned form as

\bee
\Abf = \Qbf \Rbf = \Qbf \begin{bmatrix} \Rbf_1 \\ \zerobf \end{bmatrix} = \begin{bmatrix} \Qbf_1 & \Qbf_2 \end{bmatrix} \begin{bmatrix} \Rbf_1 \\ \zerobf \end{bmatrix} = \Qbf_1 \Rbf_1
\eee

where $\Rbf_1$ is an $n \times n$ upper triangular matrix, $\zerobf$ is an $m - n \times n$ zero matrix, $\Qbf_1$ is $m \times n$, $\Qbf_2$ is $m \times m - nn$, and $\Qbf_1$ and $\Qbf_2$ both have orthogonal columns.

$\Qbf_1 \Rbf_1$ is sometimes called the \emph{reduced QR decomposition}. If we require that the diagonal elements of $\Rbf_1$ are positive then $\Rbf_1$ and $\Qbf_1$ are unique, but in general $\Qbf_2$ is not.

\subsection{Gram-Schmidt Process}

This is the simplest method for calculating the QR decomposition; however, it can have numerical problems. The idea is to start with the first column of $\Abf$, normalize it and thereby obtain $\qbf_1$,

\bee
\qbf_1 = \frac{\abf_1}{|| \abf_1 ||}
\eee

In the next step, we project $\abf_2$ onto $\qbf_1$ and subtract the result from $\abf_2$. Normalizing yields $\qbf_2$,

\bee
\ubf_2 = \abf_2 - \text{proj}_{q_1} \abf_2, \,\, \qbf_2 = \frac{\ubf_2}{|| \ubf_2 ||}
\eee

where the ``projection operator'' projects vector $\abf$ onto direction $\ubf$ according to

\bee
\text{proj}_{u} \abf = \frac{\langle \ubf, \abf \rangle}{\langle \ubf, \ubf \rangle} \ubf
\eee

Sidenote: The ``other part'' of projection of $\abf$ onto $\qbf$ is given by 

\bee
\abf - \langle \abf, \qbf \rangle \qbf
\eee

where we have assumed that $\qbf$ has unit-length. This vector is orthogonal to $\qbf$,

\bee
\langle \qbf, \abf - \langle \abf, \qbf \rangle \qbf \rangle = \langle \qbf, \abf \rangle - \langle \qbf, (\langle \abf, \qbf \rangle) \qbf \rangle = \langle \qbf, \abf \rangle - (\langle \abf, \qbf \rangle) \langle \qbf, \qbf \rangle =  \langle \qbf, \abf \rangle - \langle \abf, \qbf \rangle = 0
\eee

where we used $\langle \qbf, \qbf \rangle = 1$. \qed

We can follow this process in an iterative manner, by using the component of $\abf_k$ which is orthogonal to the basis vectors $\ubf_1, \ldots,\ubf_{k-1}$ already obtained

\bee
\ubf_k = \abf_k - \sum_{j=1}^{k-1} \text{proj}_{q_j} \abf_k, \,\, \qbf_k = \frac{\ubf_k}{|| \ubf_k ||}
\eee

Going backwards, we can now express the $\abf_i$s in terms of the orthogonal basis $\qbf_1, \ldots, \qbf_k$,

\begin{align*}
  \abf_1 &= \langle \qbf_1, \abf_1 \rangle \qbf_1 \\
  \abf_2 &= \langle \qbf_1, \abf_2 \rangle \qbf_1 + \langle \qbf_2, \abf_2 \rangle \qbf_2 \\
  & \cdots \\
  \abf_k & = \sum_j \langle \qbf_j, \abf_k \rangle \qbf_j
\end{align*}

If we collect the vectors $\qbf_i$ in a matrix $\Qbf$, then we can write this as

\bee
\Abf = \begin{pmatrix} \qbf_1 \qbf_2 \cdots \qbf_n \end{pmatrix} \begin{pmatrix} \langle \ebf_1, \abf_1 \rangle & \langle \ebf_1, \abf_2 \rangle & \langle \ebf_1, \abf_3 \rangle & \cdots \\ 0 & \langle \ebf_2, \abf_2 \rangle & \langle \ebf_3, \abf_2 \rangle & \cdots \\ 0 & 0 & \langle \ebf_3, \abf_3 \rangle & \cdots \\ \vdots & \vdots & \vdots & \vdots \end{pmatrix} = \Qbf \Rbf
\eee

and this is exactely the QR factorization.

\subsection{Householder Transformations}

The Householder Transform generates a matrix which when multiplied with a vector can zero all components but one.

Recall from previous entries that a matrix $\Pbf_v$ projecting a vector onto the subspace spanned by a vector $\vbf$ is given by

\bee
\Pbf_v = \frac{\vbf \vbf^H}{\vbf^H \vbf}
\eee

The complementary projection $\Pbf_v^\perp$ is given by

\bee
\Pbf_v^\perp = \Ibf - \Pbf_v
\eee

A Householder Transform matrix $\Hbf_v$ with respect to a \emph{Householder vector} $\vbf$ is defined as

\bee
\Hbf_v = \Ibf - 2 \Pbf_v = \Ibf - 2 \frac{\vbf \vbf^H}{\vbf^H \vbf}
\eee

It is not a projection matrix as we have

\bee
\Hbf_v^2 = (\Ibf - 2 \Pbf_v)^2 = \Ibf - 4 \Pbf_v + 4 \Pbf_v^2 = \Ibf - 4 \Pbf_v + 4 \Pbf_v = \Ibf \neq \Hbf_v
\eee

but applying $\Hbf_v$ twice leaves a vector unchanged.

The Householder transform reflects a vector $\xbf$ along the direction of $\Pbf_v^\perp \xbf$. We can write

\bee
\xbf = \Pbf_v \xbf + \Pbf_v^\perp \xbf
\eee

and if we apply the Householder transform, we obtain

\begin{align*}
  \Hbf_v \xbf &= (\Ibf - 2 \Pbf_v) (\Pbf_v \xbf + \Pbf_v^\perp \xbf) = \Pbf_v \xbf + \Pbf_v^\perp \xbf - 2 \Pbf_v (\Pbf_v \xbf + \Pbf_v^\perp \xbf) = \\
  &= \Pbf_v \xbf + \Pbf_v^\perp \xbf - 2 \Pbf_v \xbf - 2 \Pbf_v \Pbf_v^\perp \xbf = \Pbf_v^\perp \xbf - \Pbf_v \xbf
\end{align*}

using the fact that $\Pbf_v \Pbf_v^\perp \xbf = \Pbf_v (\Ibf - \Pbf_v) \xbf = (\Pbf_v - \Pbf_v) \xbf = \zerobf$. This makes intuitively sense, as projecting onto $\vbf$ first and then onto the complement should yield nothing. So we have

\bee
\Hbf_v \xbf = \Pbf_v^\perp \xbf - \Pbf_v \xbf
\eee

The $\xbf$-component perpendicular to $\vbf$ is not changed; the component in the direction of $\vbf$ is mirrored. In addition, we see that $\Hbf_v$ does not change the length of the vector; i.e.

\bee
|| \Hbf_v \xbf ||^2 = || \xbf ||^2
\eee

A picture of the involved vectors and projections is shown in the following Figure.

\begin{figure}[hbt!]
\centering
\includegraphics[scale=0.85]{images/num_lin_alg_06_01.png}
\end{figure}

Now let's take a vector $\xbf=[x_1 ,\ldots x_n]^T$ and apply the Householder transform to it,

\bee
\Hbf_v \begin{pmatrix} x_1 \\ x_2 \\ \cdots \\ x_n \end{pmatrix} = \begin{pmatrix} \alpha \\ 0 \\ \cdots \\ 0 \end{pmatrix} = \alpha \ebf_1
\eee

Since the Householder transform does not change length, we have $\alpha = \pm || \xbf||$. The question is how to choose $\vbf$ from above equation. We have

\bee
\left( \Ibf - 2 \frac{\vbf \vbf^H}{\vbf^H \vbf} \right) \xbf = \alpha \ebf_1
\eee

and from this we obtain

\bee
\xbf - 2 \frac{\vbf \vbf^H}{\vbf^H \vbf} \xbf = \alpha \ebf_1 \rightarrow 2 \frac{\vbf^H \xbf}{\vbf^H \vbf} \vbf = \xbf - \alpha \ebf_1
\eee

This implies that $\vbf$ is a scalar multiple of $\xbf - \alpha \ebf_1$ and we can choose

\bee
\vbf = \xbf \pm || \xbf || \ebf_1
\eee

The whole procedure is shown in the following Figure.

\begin{figure}[hbt!]
\centering
\includegraphics[scale=0.35]{images/num_lin_alg_06_02.png}
\end{figure}


\paragraph{Example.} We take the vector $\xbf=[1, 4, 2]^T$ and calculate the Householder Transform in Julia as follows

\begin{verbatim}
x=[1;4;2]
v=x + norm(x)*[1;0;0]
3-element Array{Float64,1}:
 5.58257569495584
 4.0             
 2.0             
Hv=diagm(0 =>[1,1,1])-2*v*v'/(v'*v)
3Ã—3 Array{Float64,2}:
 -0.218218  -0.872872  -0.436436
 -0.872872   0.374574  -0.312713
 -0.436436  -0.312713   0.843644
Hv*x
3-element Array{Float64,1}:
 -4.58257569495584      
 -1.1102230246251565e-16
  0.0
norm(Hv*x)
4.58257569495584
norm(x)
4.58257569495584

\end{verbatim}


\paragraph{Application to QR Factorization.} The idea is rather simple; we subsequently zero out elements in the columns from left to right. These matrices are denoted $\Qbf_1, \Qbf_2, \ldots$.

As illustration, we consider a $4 \times 3$ matrix $\Abf$. In a first step, we multiply with a appropriately chosen Householder matrix $\Hbf_1 = \Qbf_1$ to obtain

\bee
\Hbf_1 \Abf = \begin{pmatrix} \alpha_1 & x & x \\ 0 & x & x \\ 0 & x & x \\ 0 & x & x \end{pmatrix}
\eee

In this paragraph, we note non-zero elements with $x$. We next tackle the second column and zero all but the first two elements. This is achieved by matrix-multiplication with $\Qbf_2$ chosen as

\bee
\Qbf_2 = \begin{pmatrix} 1 & 0 \\ \Hbf_2 & 0 \end{pmatrix}
\eee

where $\Hbf_2$ is an appropriately chosen Householder matrix. Multiplying everything together yields

\bee
\Qbf_2 \Qbf_1 \Abf = \begin{pmatrix} \alpha_1 & x & x \\ 0 & \alpha_2  & x \\ 0 & 0 & x \\ 0 & 0 & x \end{pmatrix}
\eee

Applying the procedure a third time yields

\bee
\Qbf_3 \Qbf_2 \Qbf_1 \Abf = \begin{pmatrix} \alpha_1 & x & x \\ 0 & \alpha_2  & x \\ 0 & 0 & \alpha_3 \\ 0 & 0 & 0 \end{pmatrix}
\eee

with

\bee
\Qbf_3 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \Hbf_3 \end{pmatrix}
\eee

Denote the last matrix as $\Rbf$ and thereby we arrive at

\bee
\Qbf_3 \Qbf_2 \Qbf_1 \Abf = \Rbf
\eee

One can show that the $\Qbf_i$s are orthogonal and so is their product. We therefore have

\bee
\Abf = \Qbf_1 \Qbf_2 \Qbf_3 \Rbf
\eee



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "journal"
%%% End:
