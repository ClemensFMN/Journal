\DiaryEntry{Linear Algebra - QR Decomposition}{2019-11-22}{Linear Algebra}

The underlying idea of the QR decomposition is to factor a matrix $\Abf$ into $\Abf = \Qbf \Rbf$, where $\Qbf$ is an othogonal matrix and $\Rbf$ is an upper triangular matrix.

Writing $\Abf$ as a product $\Qbf \Rbf$ allows solving a linear equation system in a numercially stabel way: The equation $\Abf \xbf = \abf$ becomes $\Qbf \Rbf \xbf = \abf$ and from this we obtain $\Rbf \xbf = \Qbf^T \abf$ and this equation can be solved easily via back-substitution.


\paragraph{Square Matrix.} We have $\Abf = \Qbf \Rbf$ with matrix $\Abf$ being an $n \times n$ matrix. If $\Abf$ is invertible (and therefore full-rank), then the factorization is unique if we require the diagonal elements of $\Rbf$ to be positive.

If $\Abf$ has $n$ linearly independent columns, then the first $n$ columns of $\Qbf$ form an orthonormal basis for the column space of $\Abf$. More generally, the first $k$ columns of $\Qbf$ form an orthonormal basis for the span of the first $k$ columns of $\Abf$ for any $1 \leq k \leq n$. The fact that any column $k$ of $\Abf$ only depends on the first $k$ columns of $\Qbf$ is responsible for the triangular form of $\Rbf$.

\paragraph{Rectangular Matrix.} In this case we can factor an $m \times n$ matrix $\Abf$, with $m \geq n$, as the product of an $m \times m$ unitary matrix $\Qbf$ and an $m \times n$ upper triangular matrix $\Rbf$. The bottom $m-n$ rows of $\Rbf$ consist of zeros; we therefore can write the QR decomposition in a partitioned form as

\bee
\Abf = \Qbf \Rbf = \Qbf \begin{bmatrix} \Rbf_1 \\ \zerobf \end{bmatrix} = \begin{bmatrix} \Qbf_1 & \Qbf_2 \end{bmatrix} \begin{bmatrix} \Rbf_1 \\ \zerobf \end{bmatrix} = \Qbf_1 \Rbf_1
\eee

where $\Rbf_1$ is an $n \times n$ upper triangular matrix, $\zerobf$ is an $m - n \times n$ zero matrix, $\Qbf_1$ is $m \times n$, $\Qbf_2$ is $m \times m - nn$, and $\Qbf_1$ and $\Qbf_2$ both have orthogonal columns.

$\Qbf_1 \Rbf_1$ is sometimes called the \emph{reduced QR decomposition}. If we require that the diagonal elements of $\Rbf_1$ are positive then $\Rbf_1$ and $\Qbf_1$ are unique, but in general $\Qbf_2$ is not.

\subsection{Gram-Schmidt Process}

This is the simplest method for calculating the QR decomposition; however, it can have numerical problems. The idea is to start with the first column of $\Abf$, normalize it and thereby obtain $\qbf_1$,

\bee
\qbf_1 = \frac{\abf_1}{|| \abf_1 ||}
\eee

In the next step, we project $\abf_2$ onto $\qbf_1$ and subtract the result from $\abf_2$. Normalizing yields $\qbf_2$,

\bee
\ubf_2 = \abf_2 - \text{proj}_{q_1} \abf_2, \,\, \qbf_2 = \frac{\ubf_2}{|| \ubf_2 ||}
\eee

where the ``projection operator'' projects vector $\abf$ onto direction $\ubf$ according to

\bee
\text{proj}_{u} \abf = \frac{\langle \ubf, \abf \rangle}{\langle \ubf, \ubf \rangle} \ubf
\eee

Sidenote: The ``other part'' of projection of $\abf$ onto $\qbf$ is given by 

\bee
\abf - \langle \abf, \qbf \rangle \qbf
\eee

where we have assumed that $\qbf$ has unit-length. This vector is orthogonal to $\qbf$,

\bee
\langle \qbf, \abf - \langle \abf, \qbf \rangle \qbf \rangle = \langle \qbf, \abf \rangle - \langle \qbf, (\langle \abf, \qbf \rangle) \qbf \rangle = \langle \qbf, \abf \rangle - (\langle \abf, \qbf \rangle) \langle \qbf, \qbf \rangle =  \langle \qbf, \abf \rangle - \langle \abf, \qbf \rangle = 0
\eee

where we used $\langle \qbf, \qbf \rangle = 1$. \qed

We can follow this process in an iterative manner, by using the component of $\abf_k$ which is orthogonal to the basis vectors $\ubf_1, \ldots,\ubf_{k-1}$ already obtained

\bee
\ubf_k = \abf_k - \sum_{j=1}^{k-1} \text{proj}_{q_j} \abf_k, \,\, \qbf_k = \frac{\ubf_k}{|| \ubf_k ||}
\eee

Going backwards, we can now express the $\abf_i$s in terms of the orthogonal basis $\qbf_1, \ldots, \qbf_k$,

\begin{align*}
  \abf_1 &= \langle \qbf_1, \abf_1 \rangle \qbf_1 \\
  \abf_2 &= \langle \qbf_1, \abf_2 \rangle \qbf_1 + \langle \qbf_2, \abf_2 \rangle \qbf_2 \\
  & \cdots \\
  \abf_k & = \sum_j \langle \qbf_j, \abf_k \rangle \qbf_j
\end{align*}

If we collect the vectors $\qbf_i$ in a matrix $\Qbf$, then we can write this as

\bee
\Abf = \begin{pmatrix} \qbf_1 \qbf_2 \cdots \qbf_n \end{pmatrix} \begin{pmatrix} \langle \ebf_1, \abf_1 \rangle & \langle \ebf_1, \abf_2 \rangle & \langle \ebf_1, \abf_3 \rangle & \cdots \\ 0 & \langle \ebf_2, \abf_2 \rangle & \langle \ebf_3, \abf_2 \rangle & \cdots \\ 0 & 0 & \langle \ebf_3, \abf_3 \rangle & \cdots \\ \vdots & \vdots & \vdots & \vdots \end{pmatrix} = \Qbf \Rbf
\eee

and this is exactely the QR factorization.

\subsection{Householder Transformations}

The Householder Transform generates a matrix which when multiplied with a vector can zero all components but one.

Recall from previous entries that a matrix $\Pbf_v$ projecting a vector onto the subspace spanned by a vector $\vbf$ is given by

\bee
\Pbf_v = \frac{\vbf \vbf^H}{\vbf^H \vbf}
\eee

The complementary projection $\Pbf_v^\perp$ is given by

\bee
\Pbf_v^\perp = \Ibf - \Pbf_v
\eee

A Householder Transform matrix $\Hbf_v$ with respect to a \emph{Householder vector} $\vbf$ is defined as

\bee
\Hbf_v = \Ibf - 2 \Pbf_v = \Ibf - 2 \frac{\vbf \vbf^H}{\vbf^H \vbf}
\eee

It is not a projection matrix as we have

\bee
\Hbf_v^2 = (\Ibf - 2 \Pbf_v)^2 = \Ibf - 4 \Pbf_v + 4 \Pbf_v^2 = \Ibf - 4 \Pbf_v + 4 \Pbf_v = \Ibf \neq \Hbf_v
\eee

but applying $\Hbf_v$ twice leaves a vector unchanged.

The Householder transform reflects a vector $\xbf$ along the direction of $\Pbf_v^\perp \xbf$. We can write

\bee
\xbf = \Pbf_v \xbf + \Pbf_v^\perp \xbf
\eee

and if we apply the Householder transform, we obtain

\begin{align*}
  \Hbf_v \xbf &= (\Ibf - 2 \Pbf_v) (\Pbf_v \xbf + \Pbf_v^\perp \xbf) = \Pbf_v \xbf + \Pbf_v^\perp \xbf - 2 \Pbf_v (\Pbf_v \xbf + \Pbf_v^\perp \xbf) = \\
  &= \Pbf_v \xbf + \Pbf_v^\perp \xbf - 2 \Pbf_v \xbf - 2 \Pbf_v \Pbf_v^\perp \xbf = \Pbf_v^\perp \xbf - \Pbf_v \xbf
\end{align*}

using the fact that $\Pbf_v \Pbf_v^\perp \xbf = \Pbf_v (\Ibf - \Pbf_v) \xbf = (\Pbf_v - \Pbf_v) \xbf = \zerobf$. This makes intuitively sense, as projecting onto $\vbf$ first and then onto the complement should yield nothing. So we have

\bee
\Hbf_v \xbf = \Pbf_v^\perp \xbf - \Pbf_v \xbf
\eee

The $\xbf$-component perpendicular to $\vbf$ is not changed; the component in the direction of $\vbf$ is mirrored. In addition, we see that $\Hbf_v$ does not change the length of the vector; i.e.

\bee
|| \Hbf_v \xbf ||^2 = || \xbf ||^2
\eee

A picture of the involved vectors and projections is shown in the following Figure.

\begin{figure}[hbt!]
\centering
\includegraphics[scale=0.85]{images/num_lin_alg_06_01.png}
\end{figure}

Now let's take a vector $\xbf=[x_1 ,\ldots x_n]^T$ and apply the Householder transform to it,

\bee
\Hbf_v \begin{pmatrix} x_1 \\ x_2 \\ \cdots \\ x_n \end{pmatrix} = \begin{pmatrix} \alpha \\ 0 \\ \cdots \\ 0 \end{pmatrix} = \alpha \ebf_1
\eee

Since the Householder transform does not change length, we have $\alpha = \pm || \xbf||$. The question is how to choose $\vbf$ from above equation. We have

\bee
\left( \Ibf - 2 \frac{\vbf \vbf^H}{\vbf^H \vbf} \right) \xbf = \alpha \ebf_1
\eee

and from this we obtain

\bee
\xbf - 2 \frac{\vbf \vbf^H}{\vbf^H \vbf} \xbf = \alpha \ebf_1 \rightarrow 2 \frac{\vbf^H \xbf}{\vbf^H \vbf} \vbf = \xbf - \alpha \ebf_1
\eee

This implies that $\vbf$ is a scalar multiple of $\xbf - \alpha \ebf_1$ and we can choose

\bee
\vbf = \xbf \pm || \xbf || \ebf_1
\eee

The whole procedure is shown in the following Figure.

\begin{figure}[hbt!]
\centering
\includegraphics[scale=0.35]{images/num_lin_alg_06_02.png}
\end{figure}


\paragraph{Example.} We take the vector $\xbf=[1, 4, 2]^T$ and calculate the Householder Transform in Julia as follows

\begin{verbatim}
x=[1;4;2]
v=x + norm(x)*[1;0;0]
3-element Array{Float64,1}:
 5.58257569495584
 4.0             
 2.0             
Hv=diagm(0 =>[1,1,1])-2*v*v'/(v'*v)
3Ã—3 Array{Float64,2}:
 -0.218218  -0.872872  -0.436436
 -0.872872   0.374574  -0.312713
 -0.436436  -0.312713   0.843644
Hv*x
3-element Array{Float64,1}:
 -4.58257569495584      
 -1.1102230246251565e-16
  0.0
norm(Hv*x)
4.58257569495584
norm(x)
4.58257569495584

\end{verbatim}


\paragraph{Application to QR Factorization.} The idea is rather simple; we subsequently zero out elements in the columns from left to right using Householder transformations. The Householder matrices are denoted $\Hbf_1, \Hbf_2, \ldots$, the matrices the Householder matrices are embedded into are denoted $\Qbf_1, \Qbf_2, \ldots$.

As illustration, we consider a $4 \times 3$ matrix $\Abf$. In a first step, we multiply with a appropriately chosen Householder matrix $\Hbf_1 = \Qbf_1$ to obtain

\bee
\Hbf_1 \Abf = \begin{pmatrix} \alpha_1 & x & x \\ 0 & x & x \\ 0 & x & x \\ 0 & x & x \end{pmatrix}
\eee

In this paragraph, we note non-zero elements with $x$. We next tackle the second column and zero all but the first two elements. This is achieved by matrix-multiplication with $\Qbf_2$ chosen as

\bee
\Qbf_2 = \begin{pmatrix} 1 & 0 \\ \Hbf_2 & 0 \end{pmatrix}
\eee

where $\Hbf_2$ is an appropriately chosen Householder matrix. Multiplying everything together yields

\bee
\Qbf_2 \Qbf_1 \Abf = \begin{pmatrix} \alpha_1 & x & x \\ 0 & \alpha_2  & x \\ 0 & 0 & x \\ 0 & 0 & x \end{pmatrix}
\eee

Applying the procedure a third time yields

\bee
\Qbf_3 \Qbf_2 \Qbf_1 \Abf = \begin{pmatrix} \alpha_1 & x & x \\ 0 & \alpha_2  & x \\ 0 & 0 & \alpha_3 \\ 0 & 0 & 0 \end{pmatrix}
\eee

with

\bee
\Qbf_3 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \Hbf_3 \end{pmatrix}
\eee

Denote the last matrix as $\Rbf$ and thereby we arrive at

\bee
\Qbf_3 \Qbf_2 \Qbf_1 \Abf = \Rbf
\eee

One can show that the $\Qbf_i$s are orthogonal and so is their product. We therefore have

\bee
\Abf = \Qbf_1 \Qbf_2 \Qbf_3 \Rbf
\eee

\paragraph{Example.} Let's make this more clear by stepping through an exmaple with

\bee
\Abf =\begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ 10 & 11 & 15 \end{pmatrix}
\eee

For zeroing the first column we have the following Julia commands

\begin{verbatim}
x1 = A[:,1]
v1 = x1 + norm(x1)*[1,0,0,0]
H1 = diagm(0 => [1,1,1,1])-2*v1*v1'/(v1'*v1)

H1*A
4Ã—3 Array{Float64,2}:
 -12.8841       -14.5916    -18.6276  
   4.44089e-16    0.219962   -0.230901
   8.88178e-16   -0.365067   -1.90408 
   8.88178e-16   -0.950095   -0.577254
\end{verbatim}

We continue with the second column

\begin{verbatim}
x2 = (H1*A)[2:4,2]
v2 = x2+norm(x2)*[1;0;0]
H2 = diagm(0 => [1,1,1])-2*v2*v2'/(v2'*v2)
Q2 = zeros(4,4)
Q2[1,1] = 1
Q2[2:4,2:4] = H2

Q2*H1*A
4Ã—3 Array{Float64,2}:
 -12.8841       -14.5916       -18.6276  
   1.33227e-15   -1.04132       -1.14545 
   8.88178e-16    0.0           -1.63937 
   2.22045e-16   -7.77156e-16    0.111655
\end{verbatim}

and finally, the third one

\begin{verbatim}
x3 = (Q2*H1*A)[3:4,3]
v3 = x3+norm(x3)*[1;0]
H3 = diagm(0 => [1,1])-2*v3*v3'/(v3'*v3)
Q3 = zeros(4,4)
Q3[1,1] = 1
Q3[2,2] = 1
Q3[3:4,3:4] = H3

Q3*Q2*H1*A
4Ã—3 Array{Float64,2}:
 -12.8841       -14.5916       -18.6276     
   1.33227e-15   -1.04132       -1.14545    
   0.0            0.0           -1.64317    
   9.12488e-17    5.00054e-16   -3.63789e-15

H1*Q2*Q3
4Ã—4 Array{Float64,2}:
 -0.0776151  -0.833052  -0.365148   0.408248   
 -0.31046    -0.451237   0.182574  -0.816497   
 -0.543305   -0.069421   0.730297   0.408248   
 -0.776151    0.312395  -0.547723  -1.80906e-15
\end{verbatim}

We can compare with Julia's built-in qr function which yields

\begin{verbatim}
qr(A)
LinearAlgebra.QRCompactWY{Float64,Array{Float64,2}}

Q factor:
4Ã—4 LinearAlgebra.QRCompactWYQ{Float64,Array{Float64,2}}:
 -0.0776151  -0.833052   0.365148  -0.408248   
 -0.31046    -0.451237  -0.182574   0.816497   
 -0.543305   -0.069421  -0.730297  -0.408248   
 -0.776151    0.312395   0.547723  -1.11022e-15

R factor:
3Ã—3 Array{Float64,2}:
 -12.8841  -14.5916   -18.6276 
   0.0      -1.04132   -1.14545
   0.0       0.0        1.64317
\end{verbatim}

So the $\Qbf_3 \Qbf_2 \Hbf_1 \Abf$ product equals the $\Rbf$-component and the $\Hbf_1 \Qbf_2 \Qbf_3$ equals the $\Qbf$-component. Julia seems to choose the signs slightly different, resulting in slight differences.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "journal"
%%% End:
