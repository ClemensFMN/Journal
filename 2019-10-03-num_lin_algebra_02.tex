\DiaryEntry{Numerical Linear Algebra, 2}{2019-10-03}{Numerical Algebra}

\subsection{Vector Norms}

A \emph{norm} is a function $|| \cdot || : \mC^m \rightarrow \mC$ mapping a vector to a real number. In order for a function to be a norm, it must fulfill the following three conditions ($\xbf, \ybf$ are vectors, $\alpha$ is a scalar)

\begin{align*}
  & || \xbf || \geq 0, || \xbf || = 0 \quad \text{iff} \quad \xbf = \zerobf \\
  & || \xbf + \ybf || \leq || \xbf || + || \ybf || \\
  & || \alpha \xbf || = |\alpha| || \xbf ||
\end{align*}

There are several norms widely used. The $1$-norm is defined as

\bee
|| \xbf ||_1 = \sum_i | x_i |
\eee

The ``classical'' $2$-norm is

\bee
|| \xbf ||_2 = \sum_i | x_i |^2
\eee

The inifinity-norm is

\bee
|| \xbf ||_\infty = \max_i | x_i |
\eee

A generalization is the $p$-norm which is given by

\bee
|| \xbf ||_p = \left( \sum_i | x_i |^p \right)^{1/p}
\eee

We can also define weighted $p$-norms,

\bee
|| \xbf ||_W = || \Wbf \xbf ||
\eee

with a diagonal matrix $\Wbf$ in which the $i$-th diagonal entry is the weight $w_i \neq 0$. For example, a weighted $2$-norm is given by

\bee
|| \xbf ||_W = \left( \sum_i | w_i x_i |^2 \right)^{1/2}
\eee

The following Figure shows the norms and the area for which the norm is below a fixed value.

\begin{figure}[hbt!]
\centering
\includegraphics[scale=0.5]{images/num_lin_alg_2_1.png}
\end{figure}


\subsection{Matrix Norms induced by Vector Norms}

We are (mostly) interested in matrix norms \emph{induced} by vector norms. Consider an $m \times n$ matrix $\Abf$ and two vector norms $||\cdot||_{(m)}$ and $||\cdot||_{(n)}$. The induced matrix norm $||\Abf ||_{(m,n)}$ is then the smallest number $C$ for which the following inequality holds for all $\xbf \in \mC^n$

\bee
||\Abf \xbf ||_{(m)} \leq C || \xbf ||_{(n)}
\eee

In other words, the induced matrix norm is the supremum of the ratios $||\Abf \xbf ||_{(m)} / ||\xbf ||_{(n)}$ over all vectors $\xbf$; i.e. the maximum factor by which $\Abf$ can stretch a vector $\xbf$. Since everything is linear, we can restrict ourselves to unit vectors and obtain

\bee
\Abf ||_{(m,n)} = \sup_{\xbf \in \mC^m, || \xbf||_{(n)} = 1} ||\Abf \xbf ||_{(m)}
\eee

Note that there are two norms in the expression; one for the vector $\xbf||_{(n)}$ and one for the matrix-vector product $||\Abf \xbf ||_{(m)}$. 

In case of $2$-norms, the induced matrix norm is equal the largest singular value of $\Abf$.

\paragraph{Example.} We consider a two-dimensional case. Here we can parametrize the vector $\xbf$ as

\bee
\xbf = [\cos \phi \, \sin \phi]^T
\eee

and then finding the supremum becomes easier as we have a one-dimensional optimization problem. For example,

\bee
\Abf = \begin{pmatrix} 1 & 0 \\ 0 & 3 \end{pmatrix}, \quad \Abf \xbf = [\cos \phi \;\; 3\sin\phi ]^T
\eee

and the norm becomes

\bee
|| \Abf \xbf || = \sqrt{\cos^2 \phi + 9\sin^2\phi}
\eee

The expression reaches its maximum value of $3$ for $\phi = \pi/2$. Therefore the induced matrix norm is $3$ which is consistent with the largest singular value of $\Abf$ being $3$.The corresponding vector is $\xbf = [0 \;\; 1]^T$ and this makes intuitively sense, as the maxtrix $\Abf$ strongly amplifies the second component.

We can do this with a more complex matrix as well,

\bee
\Abf = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}, \quad \Abf \xbf = [\cos \phi + 2 \sin\phi \;\; 3\cos\phi+4\sin\phi ]^T
\eee

We can solve this and obtain for the induced norm $5.465$; this matches with the largest singular value. The corresponding vector is $[0.576 \;\; 0.817]^T$.


\subsection{Special Cases}

\paragraph{$p$-Norm of a Diagonal Matrix.} Let the diagonal matrix be

\bee
\Dbf = \text{diag}(d_1 d_2 \cdots d_m)
\eee

then the norm is given by $|| \Dbf ||_p = \max_i d_i$.

\paragraph{$1$-Norm of a Matrix.} We write $\Abf$ in terms of its columns;

\bee
\Abf = \left[ \abf_1 \abf_2 \cdots \abf_n \right]
\eee

with $\abf_i$ being the $i$-th column. The $1-$ norm is then

\bee
|| \Abf ||_1 = \max_i \abf_i
\eee

\subsection{Cauchy-Schwarz and Hölder Inequalities}

Let $p$ and $q$ satisfy $1/p + 1/q = 1$ with $1 \leq p,q \leq \infty$. The \emph{Hölder inequality} states that , for any vectors $\xbf, \ybf$,

\bee
|\xbf^T \ybf| \leq || \xbf||_p || \xbf||_q
\eee

For the special case $p = q = 1/2$ we obtain the \emph{Cauchy-Schwarz inequality},

\bee
|\xbf^T \ybf| \leq || \xbf||_2 || \xbf||_2
\eee

\subsection{Hilbert-Schmidt or Frobenius Norm}

This norm is not induced by a vector norm but considers the matrix as consisting of $mn$ entries. The norm fulfills the three properties of norms

\begin{align*}
  & || \Xbf || \geq 0, || \Xbf || = 0 \quad \text{iff} \quad \Xbf = \zerobf \\
  & || \Xbf + \Ybf || \leq || \Xbf || + || \Ybf || \\
  & || \alpha \Xbf || = |\alpha| || \Xbf ||
\end{align*}

and is given by

\bee
|| \Abf ||_F = \left( \sum_i \sum_j |a_{ij}^2| \right)^{1/2}
\eee

This is the same as th $2$-norm of the matrix when viewed as vector of $mn$ entries. We can express the Forobenius norm also in terms of columns and traces as below

\bee
|| \Abf ||_F = \left( \sum_i ||\abf_{i}||_2^2 \right)^{1/2} = \sqrt{ \text{tr}(\Abf^T \Abf)}
\eee

If we have two matrix $\Abf, \Bbf$, we can bound the Frobenius norm of their product $\Cbf = \Abf \Bbf$. Let $\abf_i^T$ denote the $i$-th row of $\Abf$ and $\bbf_i$ the $i$-th column of $\Bbf$, then $c_{ij} = \abf_i^T \bbf_j$ and by Cauchy-Schwarz, we have

\bee
|c_{ij}| \leq || \abf_i||_2 || \bbf_j||_2
\eee

Squaring both sides and summing acrss $i, j$, we obtain

\bee
\sum_{ij} |c_{ij}|^2 = || \Cbf ||_F^2 \leq \sum_{ij} || (\abf_i||_2)^2 (|| \bbf_j||_2)^2 = \sum_i (|| \abf_i||_2)^2 \sum_i (|| \bbf_i||_2)^2 = || \Abf ||_F^2 || \Bbf ||_F^2
\eee

Finally note that the $2$-norm and Frobenius norm are invariant under multiplication with unitary matrices (denoted $\Qbf$),

\bee
|| \Qbf \Abf ||_2 = || \Abf ||_2, \quad || \Qbf \Abf ||_F = || \Abf ||_F
\eee


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "journal"
%%% End:
