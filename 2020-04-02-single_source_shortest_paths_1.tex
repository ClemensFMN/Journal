\DiaryEntry{Single-Source Shortest Paths}{2020-04-02}{Algorithms}

\subsection{Introduction}

We are given a weighted, directed graph $G=(V,E)$ with a weight function $w: E \rightarrow \mR$ mapping edges to real-valued weights. A path $p$ is a sequence of edges, $p=\langle v_0,v_,\cdots,v_n \rangle$; its \emph{weight} $w(p)$ is given by the sum of its edges

\bee
w(p) = \sum_{i=1}^n w(v_{i-1},v_i)
\eee

The \emph{shortest-path weight} $\delta(u,v)$ from vertex $u$ to vertex $v$ is defined as

\bee
\delta(u,v) = \begin{cases} \min_p w(p): u \rightarrow v \, & \text{if there is a path from u to v} \\ \infty \, & \text{otherwise} \end{cases}
\eee

A \emph{shortest path} from $u$ to $v$ is then any path from $u$ to $v$ with weight $w(p) = \delta(p)$. There can be more than one shortest path between $u$ and $v$.

In this entry, we focus on \emph{single-source shortest paths problems}; i.e. we are given one source egde $s \in V$ and determine shortest paths to all vertices $v \in V$. 

The followig theorem helps in  with finding shortest paths.

\begin{theorem}
Given a weighted graph $G=(V,E)$ with weight function $w: E \rightarrow \mR$, let $p=\langle v_0,v_,\cdots,v_n \rangle$; be a shortest path from vertex $v_0$ to vertex $v_n$, and, for any $i$ and $j$ such that $0 \leq i \leq j \leq n$, let $p_{ij} = \langle v_i,v_,\cdots,v_j \rangle$ be the subpath of $p$ from vertex $v_i$ to vertex $v_j$. Then, $p_{ij}$ is a shortest path from $i$ to $j$. 
\end{theorem}


\paragraph{Example.} We consider the following weighted graph as example.


\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{images/sssp_1.png}
\end{figure}


If we choose the source vertex $s=1$ and run a single-source shortest path algorithm (Bellman-Ford), we obtain the following result: The algorithm returns a tree (via a predecessor graph) which is shown in red in the Figure below.


\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{images/sssp_2.png}
\end{figure}

In addition, the algorithm provides the distance between the source and each vertex. This is shown in the Table below.

\begin{tabular}{c|c}
  Vertex & Distance \\ \hline
  1 & 0 \\
  2 & 1 \\
  3 & 2 \\
  4 & 2 \\
  5 & 5 \\
  6 & 6
\end{tabular}

\paragraph{Negative Weights and Cycles.} Some algorithms allow for negative edge weights. A necessary condition for a soluton is, that there are no cycles with negative weights: If there would be, then no path can be shortest (minimum weight) as appending (another) cycle to a path would create a path with even lower weight. An example of this is shown in the following Figure: There is a cycle of negative weight $-3$ between vertices $2$ and $3$. Any path between vertices $1$ and $4$ can be made shorter by running around this cylce one more time.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{images/sssp_3.png}
\end{figure}

From a general perspective, shortest paths cannot contain cycles:

\begin{itemize}
\item Cycle with negative weight are not allowed as there no shortest paths (see above).
\item Cycles with positive weight can be elimated and yield a shorter path.
\item Cycles with zero weight, do not affect the path weight, but cano be removed without changing the weight of the shortest path.
\end{itemize}


\paragraph{Relaxation.} 

For each vertex $v$ we maintain an attribute $v.d$ which is an upper bound of the weight of a shortest path from source $s$ to $v$. This attribute is called shortest-path estimate. In addition, for every vertex $v$ we have a predecessor $v.\pi$. The shortest path algorithms set this attribute accordingly; by starting at a vertex $v$ and ``tracing back'' to vertex $s$, we obtain the shortest path between $s$ and $v$.

These two attributes are initialzed as follows:

\begin{verbatim}
Init-Single-Source(G,s)
   for each vertex v in G
      v.d = INF
      v.pi = NULL
   s.d = 0
\end{verbatim}

Relaxing an edge $(u,v)$ means to test whether we can improve the shortest path to vertex $v$ found so far by going through vertex $u$. If this is the case, then we update $v.\pi$ and $v.d$.

The relaxation algorithm looks as follows

\begin{verbatim}
Relax(u,v,w)
   if v.d > u.d + w(u,v)
      v.d = u.d + w(u,v)
      v.pi = u 
\end{verbatim}

An example is shown in the following Figure. Left (a):  The upper bound is $v.d = 9$; if we choose going via vertex $u$ instead, we can tighten the bound to $v.d = u.d + w(u,v) = 5 + 2 = 7$. In the right Subfigure (b), this is not possible; going via vertex $u$ instead, would increase $v.d$ and therefore relaxation is not possible.   


\begin{figure}[H]
\centering
\includegraphics[scale=0.65]{images/sssp_4.png}
\end{figure}



\subsection{Bellman-Ford Algorithm}

The Bellman-Ford algorithm is

\begin{verbatim}
Bellman-Ford(G, w, s)
   Init-Single-Source(G,s)
   for i = 1 to |G.V|-1
      for each edge (u,v) in G.E
         Relax(u,v,w)
   for each edge (u,v) in G.E
      if v.d > u.d + w(u,v)
         return false
   return true
\end{verbatim}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "journal"
%%% End:
