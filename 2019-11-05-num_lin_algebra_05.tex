\DiaryEntry{(Numerical) Linear Algebra, Eigenvalues and Eigenvectors}{2019-11-05}{Linear Algebra}

The eigenvalues and eigenvectos are defined for square matrices only; let $\Abf \in \mR^{n \times n}$ be such a matrix. Then $\lambda \in \mR$ is an \emph{eigenvalue} and $\xbf \in \mR^n \setminus \{\zerobf\}$ is the corresponding \emph{eigenvector} of $\Abf$ if

\bee
\Abf \xbf = \lambda \xbf
\eee

Intuitively, a linear mapping as defined by $\Abf$ may change the length but not the direction of vectors with a ``special'' direction. These vectors are the eigenvectors; the stretch factor is the corresponding eigenvalue. The eigenvectors are determined only up to a constant (non-zero) factor; i.e. if $\xbf$ is an eigenvector, so is $c \xbf$ with $c \in \mR \setminus\{0\}$.

We define the characteristic equations as

\bee
\det (\Abf - \lambda \xbf) = 0
\eee

and $\lambda \in \mR$ is an eigenvalue of $\Abf$ iff $\lambda$ is a root of the characteristic equation.

The \emph{algebraic multiplicity} of $\lambda_i$ is the number of times the root appears in the characteristic equation. The set of all eigenvalues is called the \emph{eigenspectrum}. The set of all eigenvectors associated with a specific eigenvalue spans a subspace of $\mR^n$ is called eigenspace of $\Abf$ with respect to $\lambda$ and denoted by $E_\lambda$. The number of linearly independent eigenvectors associated with a specific eigenvalue is called \emph{geometric multiplicity}. It is (also) the dimension of the eigenspace $E_\lambda$.

For one specific eigenvalue, the geometric multiplicity is smaller or equal the algebraic multiplicity.

\subsection{General $\Abf$}

$\Abf$ and its transpose $\Abf^T$ have the same eigenvalues but not necessarily the same eigenvectors.

There are several cases regarding distinct eigenvalues and eigenvectors:

\begin{itemize}

  \item In case of \emph{distinct} eigenvalues, the eigenvectors will be \emph{linearly independent}, but not necessariyl orthonormal.

  \item When \emph{not} all eigenvalues are distinct, then

    \begin{itemize}

    \item There are $n$ linearly independent eigenvectors. in this case, the matrix $\Abf$ is \emph{non-defective}.

    \item There are less than $n$ linearly independent eigenvectors; the matrix $\Abf$ is \emph{defective}. 

    \end{itemize}

\end{itemize}

Eigenvectors belonging to different eigenvalues are linearly independent. \todo{proof?}

\paragraph{Example.} We have the following Julia session

\begin{verbatim}
julia> A=[-2 0 1; 1 1 0;0 0 -2]
3×3 Array{Int64,2}:
 -2  0   1
  1  1   0
  0  0  -2

julia> F=eigen(A)
Eigen{Float64,Float64,Array{Float64,2},Array{Float64,1}}
eigenvalues:
3-element Array{Float64,1}:
  1.0
 -2.0
 -2.0
eigenvectors:
3×3 Array{Float64,2}:
 0.0   0.948683  -0.948683 
 1.0  -0.316228   0.316228 
 0.0   0.0        4.213e-16

julia> F.vectors'*F.vectors
3×3 Array{Float64,2}:
  1.0       -0.316228   0.316228
 -0.316228   1.0       -1.0     
  0.316228  -1.0        1.0     

\end{verbatim}

Note that the first and second (or first and third) eigenvector are linearly independent, but the second and eigenvectors are not. Moreover, the eigenvectors are not orthogonal.

\paragraph{Eigenvectors and Eigenvalues of Specific Matrices.} In the following, we consider some special matrices and their eigenvalues and eigenvectors. We first start with a simple matrix scaling $x$-direction and $y$-direction by a different factor,

\bee
\Abf = \begin{pmatrix} 2 & 0 \\ 0 & 0.5 \end{pmatrix}
\eee

The eigenvalues and eigenvectors are

\bee
\lambda_1 = 2, \xbf=[1 0]^T, \quad \lambda_2 = 0.5, \xbf=[0 1]^T
\eee

which makes intuitively sense: Vectors in $x$- and $y$- direction are not changed; only their length changes. The determinant is $1$ and it is therefore an area-preserving transformation.

We next consider a rotation matrix by rotation angle $\phi$,

\bee
\Abf = \begin{pmatrix} \cos\phi & -\sin\phi \\ \sin\phi & \cos\phi \end{pmatrix}
\eee

Apart from the special case $\phi=0$, the eigenvalues will be complex numbers and the eigenvectors have complex components. In case of a rotation, all vectors are changed; there is no special direction of vectors which will not change their direction.

A shearing along the $x$-axis (it shear points along the $x$-axis to the right if they are on the positive half of the vertical axis and to the left otherwise) can be described by

\bee
\Abf = \begin{pmatrix} 1 & 0.5 \\ 0 & 1 \end{pmatrix}
\eee

There is an eigenvalue $1$ with algebraic multiplicity $2$ and one corresponding eigenvector $[1 0]^T$; i.e. it has geometric multiplicity $1$. This vector is not rotated by the shearing transformation. 


\subsection{Symmetric $\Abf$}

In this case we have the \emph{spectral theorem}: Each eigenvalue is real and there exists an orthonormal basis for the corresponding vector space $V$ consisting of eigenvectors of $\Abf$. Note that the eigenvectors belonging to the same eigenvalue need not be orthogonal (orthonormal) but can be made orthogonal: Assume $\xbf_1, \xbf_2$ are two eigenvectors associated with $\lambda$. Then any linear combination of these eigenvectors is again an eigenvector, 

\bee
A(\alpha \xbf_1 + \beta \xbf_2) = \lambda(\alpha \xbf_1 + \beta \xbf_2)
\eee

\paragraph{Example.} We have

\bee
\Abf = \begin{pmatrix} 3 & 2 & 2 \\ 2 & 3 & 2 \\ 2 & 2 & 3 \end{pmatrix}
\eee

The characterisitc equation is $-(\lambda-1)^2(\lambda-7) = 0$ and from this we deduce $\lambda_1 = 1, \lambda_2 = 7$, where $\lambda_1$ has arithmetic multiplicity of two. The eigenspaces are

\bee
E_1 = \text{span} \left( \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix} \right), \quad E_2 = \text{span} \left( \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} \right) 
\eee

We can use Gram-Schmidt to obtain an orthonormal base for $E_1$ as

\bee
\xbf_1' = \begin{pmatrix}-1 \\ 1 \\ 0 \end{pmatrix}, \quad \xbf_2' = \frac{1}{2} \begin{pmatrix}-1 \\ -1 \\ 2 \end{pmatrix}
\eee

This can also be done in Julia; note that Juia outputs an orthonormal base right away

\begin{verbatim}

julia> A=[3 2 2;2 3 2;2 2 3]
3×3 Array{Int64,2}:
 3  2  2
 2  3  2
 2  2  3

julia> F=eigen(A)
Eigen{Float64,Float64,Array{Float64,2},Array{Float64,1}}
eigenvalues:
3-element Array{Float64,1}:
 1.0              
 1.0              
 6.999999999999999
eigenvectors:
3×3 Array{Float64,2}:
  0.707107  -0.408248  -0.57735
 -0.707107  -0.408248  -0.57735
  0.0        0.816497  -0.57735

julia> F.vectors'*F.vectors
3×3 Array{Float64,2}:
  1.0          -5.55112e-17  -1.11022e-16
 -5.55112e-17   1.0          -2.77556e-16
 -1.11022e-16  -2.77556e-16   1.0        

\end{verbatim}

Eigenvectors belonging to different eigenvalues are orthogonal: Assume $\xbf$ and $\ybf$ are two eigenvectors belonging to eigenvales $\lambda \neq \mu$, respectively. We have

\bee
\lambda \langle \xbf, \ybf \rangle = \langle \lambda \xbf, \ybf \rangle = \langle \Abf \xbf, \ybf \rangle = \langle \xbf , \Abf^T \ybf \rangle = \langle\xbf, \Abf\ybf\rangle = \langle\xbf, \mu \ybf\rangle = \mu \langle\xbf, \ybf\rangle
\eee

Here we have used the symmetry of $\Abf$ in the fourth step. Combining we have

\bee
(\lambda - \mu) \langle\xbf, \ybf \rangle = 0
\eee

Since we assume that the eigenvaues are different, the second factor must be zero; i.e.

\bee
\langle\xbf, \ybf\rangle = 0
\eee

and the corresponding eigenvectors are orthogonal. \qed


\subsection{Eigen Decomposition}

We can collect the eigenvectors in a matrix $\Qbf$ and the corresponding eigenvalues in a diagonal matrix $\Dbf$. Then we can write

\bee
\Abf \Qbf = \Qbf \Dbf
\eee

If the matrix $\Abf$ has has $n$ linearly independent eigenvectors, then $\Qbf$ is an invertible $n \times n$ matrix and we can rewrite above equation as

\bee
\Abf = \Qbf \Dbf \Qbf^{-1}
\eee


\subsection{Connection to trace and determinant}

The trace of a matrix is the sum of its eigenvalues,

\bee
\text{tr} \Abf = \sum_i \lambda_i
\eee

and the determinant of a matrix is the product of its eigenvalues,

\bee
\det \Abf = \prod_i \lambda_i
\eee

When at least one eigenvalue is zero, the matrix is no longer full-rank, consequently its determinant becomes zero as well.

\subsection{Examples}

Consider the diagonal matrix

\bee
\Abf= \begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix}
\eee

Let's calculate the eigenvalues from the characteristic equation. We have

\bee
\det (\Abf - \lambda \Ibf) = (1-\lambda) (2 - \lambda) = 0 \rightarrow \lambda_1 = 1, \lambda_2 = 2
\eee

Next we solve $\Abf \xbf = \lambda \xbf$ for $\lambda_1 = 1$:

\bee
\begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
\eee

from which we obtain

\bee
\xbf_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}
\eee

In a similar spirit, we obtain for the other eigenvector

\bee
\xbf_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}
\eee

This is pretty intuitive; the matrix $\Abf$ does no change in the direction $[1 \, 0]^T$ and multiplies vectors by a factor of $2$ which are in the direction of $[0 \, 1]^T$. \qed

Let's see what happens in case of a matrix with dependent columns,

\bee
\Abf= \begin{pmatrix} 1 & 2 \\ 2 & 4 \end{pmatrix}
\eee

The characteristic equation and its solutions are

\bee
(1 - \lambda)(4 - \lambda) - 4 = 0 \rightarrow \lambda(\lambda - 5) = 0 \rightarrow \lambda_1 = 0, \lambda_2 = 5
\eee

The eigenvalue $\lambda_1 = 0$ yields $\Abf \xbf_1 = \zerobf$; i.e. the corresponding eigenvector lies in the nullspace of $\Abf$. We have

\bee
\begin{pmatrix} 1 & 2 \\ 2 & 4 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0\end{pmatrix}
\eee

and corresponds to $x_1 + 2x_2 = 0$ and $\xbf_1 = [t \, -t/2]^T$ with $t \in \mR$. The other eigenvector follows from

\bee
\begin{pmatrix} 1 & 2 \\ 2 & 4 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = 5 \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
\eee

as $\xbf_1 = [t \, 2t]^T$ with $t \in \mR$.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "journal"
%%% End:
